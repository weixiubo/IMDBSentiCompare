# IMDBSentiCompare ğŸ­

> A Comprehensive Sentiment Analysis Comparison Framework

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Contributions Welcome](https://img.shields.io/badge/Contributions-Welcome-brightgreen.svg)](CONTRIBUTING.md)
[![Stars](https://img.shields.io/github/stars/weixiubo/IMDBSentiCompare?style=social)](https://github.com/weixiubo/IMDBSentiCompare/stargazers)

## ğŸŒŸ Overview

IMDBSentiCompare is a comprehensive sentiment analysis research platform that systematically compares three major approaches to sentiment classification on IMDB movie reviews. This project provides a fair and thorough evaluation of different methodologies, from traditional rule-based systems to cutting-edge large language models.

### ğŸ¯ **Why IMDBSentiCompare?**

- **ğŸ“Š Comprehensive Comparison**: Framework to systematically compare rule-based, traditional ML, and LLM approaches
- **ğŸ”¬ Research-Grade**: Rigorous experimental design with proper statistical analysis
- **ğŸ› ï¸ Production-Ready**: Modular architecture suitable for both research and industry applications
- **ğŸ“ˆ Reproducible Results**: Complete pipeline with fixed random seeds and detailed documentation

## ğŸš€ Key Features

### ğŸ”§ **Rule-based Systems**
- **Multi-lexicon Fusion**: VADER + SentiWordNet + Custom dictionaries
- **Advanced Negation Handling**: Context-aware negation processing
- **Weighted Voting Mechanism**: Optimized combination of different sentiment sources

### ğŸ¤– **Traditional Machine Learning**
- **Feature Engineering**: BOW, TF-IDF with 10,000-dimensional sparse matrices
- **Feature Selection**: Chi-squared and Mutual Information with 200/2000 feature subsets
- **Model Variety**: Naive Bayes, Logistic Regression with hyperparameter optimization

### ğŸ§  **Large Language Models**
- **Multi-Strategy Prompting**: Simple, Few-shot, Chain-of-Thought approaches
- **Structured Output**: JSON-formatted analysis with topic, emotion, and intensity
- **Robust Parsing**: Error-tolerant output processing with fallback mechanisms
- **Cost Optimization**: Smart sampling strategies for API efficiency

### ğŸ“Š **Advanced Analytics**
- **Performance Metrics**: Accuracy, Precision, Recall, F1-Score
- **Statistical Analysis**: SentiWordNet correlation studies
- **Rich Visualizations**: Confusion matrices, performance comparisons, distribution plots
- **Cross-Model Validation**: Comprehensive benchmarking across all approaches

## ğŸ“ˆ Performance Results

| Model | Accuracy | Precision | Recall | F1-Score | Notes |
|-------|----------|-----------|---------|----------|-------|
| **ğŸ¥‡ LLM (Simple)** | **94.5%** | **97.0%** | **92.4%** | **94.6%** | Best overall performance |
| **ğŸ¥ˆ LLM (Few-shot)** | **93.5%** | **95.1%** | **92.4%** | **93.7%** | Consistent and reliable |
| **ğŸ¥‰ ChiÂ² (2000 features)** | **88.3%** | **87.6%** | **89.3%** | **88.4%** | Best traditional ML |
| Mutual Info (2000) | 88.0% | 87.1% | 89.2% | 88.2% | Close second in ML |
| Logistic Regression (BOW) | 87.5% | 87.4% | 87.8% | 87.6% | Solid baseline |
| Logistic Regression (TF-IDF) | 86.9% | 84.7% | 90.3% | 87.4% | High recall |
| Naive Bayes | 85.5% | 86.5% | 84.3% | 85.4% | Fast and interpretable |
| Rule-based System | 69.2% | 64.9% | 83.9% | 73.2% | High recall, lower precision |
| LLM (CoT) | 53.5% | 53.0% | 100.0% | 69.3% | An expected normal situation |

> **Note**: LLM models tested on 200 samples due to API constraints; traditional models on full test set (~5000 samples)

## ğŸ—ï¸ Project Architecture

```
IMDBSentiCompare/
â”œâ”€â”€ ğŸ“ Core Modules
â”‚   â”œâ”€â”€ config.py              # Central configuration
â”‚   â”œâ”€â”€ main.py               # Pipeline orchestrator
â”‚   â””â”€â”€ utils.py              # Shared utilities
â”œâ”€â”€ ğŸ“Š Data Processing
â”‚   â”œâ”€â”€ data_analysis.py      # EDA and statistics
â”‚   â””â”€â”€ data_preprocessing.py # Text cleaning & vectorization
â”œâ”€â”€ ğŸ”§ Classification Methods
â”‚   â”œâ”€â”€ rule_based_classifier.py    # Multi-lexicon system
â”‚   â”œâ”€â”€ ml_models.py                # Traditional ML
â”‚   â”œâ”€â”€ feature_selection.py        # Advanced feature engineering
â”‚   â””â”€â”€ llm_classifier.py           # LLM integration
â”œâ”€â”€ ğŸ“ˆ Evaluation & Analysis
â”‚   â””â”€â”€ evaluation.py         # Comprehensive evaluation suite
â”œâ”€â”€ ğŸ“‚ Data & Results
â”‚   â”œâ”€â”€ Train.csv / Valid.csv / Test.csv
â”‚   â”œâ”€â”€ models/               # Saved model artifacts
â”‚   â”œâ”€â”€ results/              # Evaluation outputs
â”‚   â””â”€â”€ figures/              # Generated visualizations
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## ğŸš€ Quick Start

### Prerequisites

```bash
# Create conda environment
conda create -n senticompare python=3.9
conda activate senticompare

# Install dependencies
conda install pandas numpy scikit-learn nltk matplotlib seaborn tqdm openai
```

### Environment Setup

```bash
# For LLM experiments (optional)
export DEEPSEEK_API_KEY="your_api_key_here"

# Download NLTK resources (automatic on first run)
python -c "import nltk; nltk.download(['punkt', 'stopwords', 'wordnet', 'vader_lexicon', 'averaged_perceptron_tagger'])"
```

### Basic Usage

```bash
# ğŸ¯ Run complete pipeline
python main.py --run-all

# ğŸ”§ Run specific components
python main.py --run-preprocessing --run-ml --run-evaluation

# ğŸ“Š Individual model training
python ml_models.py --model nb                    # Naive Bayes
python rule_based_classifier.py                   # Rule-based system
python llm_classifier.py --sample-size 200        # LLM experiments
```

### Advanced Usage

```bash
# ğŸ›ï¸ Custom feature selection
python feature_selection.py --features 500 1000 2000

# ğŸ§  LLM with different strategies
python llm_classifier.py --sample-size 100 --structured-sample-size 20

# ğŸ“ˆ Evaluation only
python evaluation.py --models-dir ./models --output-dir ./custom_results
```

## ğŸ“Š Detailed Results & Analysis

### ğŸ¯ **Key Findings**

1. **LLM Superiority**: Simple prompting strategies outperform complex Chain-of-Thought
2. **Feature Selection Impact**: 2000 features provide optimal balance of performance and efficiency
3. **Rule-based Limitations**: High recall but lower precision due to lexicon coverage gaps
4. **Cost-Performance Trade-off**: Traditional ML offers 88%+ accuracy at fraction of LLM cost

### ğŸ“ˆ **Performance Insights**

- **Best Overall**: LLM Simple strategy (94.5% accuracy)
- **Best Traditional**: Chi-squared feature selection (88.3% accuracy)
- **Most Efficient**: Naive Bayes (85.5% accuracy, fastest training)
- **Highest Recall**: Rule-based system (83.9% recall)

## ğŸ”¬ Technical Highlights

### ğŸ› ï¸ **Engineering Excellence**
- **Sparse Matrix Optimization**: Efficient handling of 10,000-dimensional features
- **Robust Error Handling**: Graceful degradation with comprehensive logging
- **Modular Design**: Easy to extend with new models or datasets
- **Reproducible Research**: Fixed random seeds and detailed configuration management

### ğŸ§ª **Research Contributions**
- **Multi-Lexicon Fusion Algorithm**: Novel weighted voting for rule-based classification
- **LLM Prompt Engineering**: Systematic evaluation of different prompting strategies
- **Fair Comparison Framework**: Handling different sample sizes and evaluation constraints
- **Comprehensive Benchmarking**: Statistical analysis with correlation studies

## ğŸ¤ Contributing

We welcome contributions from the community! Here's how you can help:

### ğŸ¯ **Priority Areas**
- [ ] Additional LLM providers (OpenAI GPT, Anthropic Claude, Google Gemini)
- [ ] New feature engineering techniques (word embeddings, transformers)
- [ ] Alternative datasets (Amazon reviews, Twitter sentiment)
- [ ] Performance optimizations (GPU acceleration, distributed computing)
- [ ] Advanced evaluation metrics (BLEU, ROUGE for structured outputs)

### ğŸ“ **How to Contribute**
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ› Known Issues & Limitations

- **LLM Sample Size**: Due to the limitations of API costs and rates, it is difficult to achieve a test set of 5,000 samples in actual scenarios
- **CoT Parsing**: The chain-of-thought strategy is not applicable to non-reasoning models
- **Memory Usage**: Large feature matrices require a large amount of memory
- **API Dependencies**: LLM experiments require stable internet connection

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **IMDB** for providing the movie reviews dataset
- **NLTK Team** for comprehensive natural language processing tools
- **DeepSeek** for providing accessible LLM API
- **Scikit-learn** for robust machine learning implementations
- **Open Source Community** for inspiration and foundational tools

## ğŸ“ Contact & Support

- **Issues**: [GitHub Issues](https://github.com/weixiubo/IMDBSentiCompare/issues)
- **Discussions**: [GitHub Discussions](https://github.com/weixiubo/IMDBSentiCompare/discussions)
- **Email**: 1494849734@qq.com

---

<div align="center">

**â­ Star this repository if you find it helpful! â­**

[ğŸ› Report Bug](https://github.com/weixiubo/IMDBSentiCompare/issues) â€¢ [âœ¨ Request Feature](https://github.com/weixiubo/IMDBSentiCompare/issues) â€¢ [ğŸ“– Documentation](https://github.com/weixiubo/IMDBSentiCompare/wiki)

</div>
